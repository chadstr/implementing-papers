{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50a5c33-1eae-47a2-b29a-dc8db0a87ebc",
   "metadata": {},
   "source": [
    "# Implementing Speculative Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147111d2-ca34-4d73-a7fa-064ea1be2a0c",
   "metadata": {},
   "source": [
    "Implementing Speculative Decoding from this paper: https://arxiv.org/pdf/2211.17192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ea451a-2ce6-4629-9e29-cd21c2cac6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea65a06-dc60-4cab-9962-1367271d3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff6f0c7-5817-4bab-b519-4243ef7246de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3351c55-39cd-4d50-bc82-6310ca52b3c9",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1718fe6c-b360-4ea4-bd41-e7fa1bf4370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tokeniser = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', device_map='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e2e6ce-cf2c-45ee-accc-6e3620de4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_tokeniser = AutoTokenizer.from_pretrained('meta-llama/Llama-2-13b-hf', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91448cb-567d-404f-bb01-59e87ca823c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0', device_map='cuda:0', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624eaecd-0697-434b-aeba-dd10894d55e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facfad65ca574ec0aac3546ff93d91f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "big_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-13b-hf', device_map='auto', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab82a3-7c59-48e6-bbf0-cd7cc2e12928",
   "metadata": {},
   "source": [
    "## Implement sampling and speculative decoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361b6a51-2368-44dc-97e3-1601330ce2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_probas(logits, k):\n",
    "    next_idxs = torch.topk(logits, k=k, dim=-1)  # (batch, context ,k)\n",
    "    min_k_values = next_idxs.values[:, :, -1]\n",
    "    min_k_values = min_k_values.unsqueeze(-1)  # (batch, context, 1)\n",
    "    top_k_logits = torch.where(\n",
    "        logits < min_k_values,\n",
    "        torch.tensor(float('-inf')).to(logits.device),\n",
    "        logits\n",
    "    )\n",
    "    probas = torch.softmax(top_k_logits, dim=-1)  # (batch, context, vocab)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e60937-6ce7-426b-b4e5-aa9506d140f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_draft(input_ids, max_new_tokens=250, k=50, end_of_turn_id=107, model=small_model):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(input_ids)['logits']\n",
    "\n",
    "        probas = get_topk_probas(logits, k)\n",
    "        last_pos_probas = probas[:, -1, :]  # (batch, vocab)\n",
    "\n",
    "        next_idx = torch.multinomial(last_pos_probas, num_samples=1)\n",
    "\n",
    "        input_ids = torch.cat((input_ids, next_idx), dim=-1)\n",
    "\n",
    "        if next_idx.item() == end_of_turn_id:\n",
    "            break\n",
    "\n",
    "    return input_ids, probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "524ffa65-3f69-4392-8e5c-ee0282e5603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_decode(input_ids, max_tokens=250, gamma=10, k=1):\n",
    "    i = input_ids.size(1)\n",
    "    num_drafts_tokens_kept = 0\n",
    "\n",
    "    while i < max_tokens:\n",
    "        num_tokens_to_gen = min(max_tokens - i, gamma)\n",
    "        draft_input_ids, draft_probas = generate_draft(input_ids, max_new_tokens=num_tokens_to_gen, k=k)\n",
    "\n",
    "        big_input_ids, big_probas = generate_draft(draft_input_ids, max_new_tokens=1, k=k, model=big_model)\n",
    "\n",
    "        resampled_id_j = None\n",
    "        for j in range(i, i + num_tokens_to_gen):\n",
    "            draft_id_j = draft_input_ids[:, j]  # (batch, 1)\n",
    "            draft_proba_j = draft_probas[torch.arange(draft_id_j.size(0)), j - 1, draft_id_j]  # j - 1\n",
    "            big_proba_j = big_probas[torch.arange(draft_id_j.size(0)), j - 1, draft_id_j]\n",
    "\n",
    "            if (draft_proba_j <= big_proba_j).item():  # If True, keep j\n",
    "                # print('keeping')\n",
    "                num_drafts_tokens_kept += 1\n",
    "                continue\n",
    "            else:\n",
    "                # Reject with probability\n",
    "                if (torch.rand(1).item() < 1 - (big_proba_j / draft_proba_j)).item():\n",
    "                    # print('rejecting')\n",
    "                    # sample again from adjusted distribution norm(max(0, p(x) - q(x)))\n",
    "                    p = big_probas[:, j - 1, :]\n",
    "                    q = draft_probas[:, j - 1, :]\n",
    "                    \n",
    "                    adjusted_p = torch.clamp(p - q, min=0)\n",
    "                    adjusted_p = adjusted_p / adjusted_p.sum()  # Normalise\n",
    "\n",
    "                    resampled_id_j = torch.multinomial(adjusted_p, num_samples=1)\n",
    "                    break\n",
    "                else:\n",
    "                    # print('keeping')\n",
    "                    num_drafts_tokens_kept += 1\n",
    "                    continue\n",
    "        if resampled_id_j is not None:\n",
    "            input_ids = torch.cat((draft_input_ids[:, : j], resampled_id_j), dim=-1)\n",
    "        else:\n",
    "            print('entire draft used')\n",
    "            input_ids = big_input_ids  # Includes last token generated by big model\n",
    "            \n",
    "        # print(big_tokeniser.decode(input_ids[0]))\n",
    "        i = input_ids.size(1)\n",
    "    return input_ids, num_drafts_tokens_kept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32011ded-9454-4b4b-b657-c508d95c6442",
   "metadata": {},
   "source": [
    "## First, examine the original output from the large and small models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5abab-6b13-4730-b292-6bf6136a37f4",
   "metadata": {},
   "source": [
    "### Large model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cf06e9a-031e-4ba3-b49b-8e7a2e07c276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   450, 16367,   310,   319, 29902,   338]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = small_tokeniser('The Future of AI is', return_tensors='pt').to('cuda:0')\n",
    "input_ids = inputs['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7588382-226e-45bb-bd50-3c3faadb77e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 3.3 s, total: 3min 2s\n",
      "Wall time: 3min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> The Future of AI is in the Hands of the People\\nThe future of AI is in the hands of the people.\\nThe future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "big_input_ids, big_probas = generate_draft(input_ids, max_new_tokens=250 - input_ids.size(1), k=1, model=big_model)\n",
    "big_tokeniser.decode(big_input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd26064-31a0-4519-a080-6b72e0591faf",
   "metadata": {},
   "source": [
    "### Small (draft) model output\n",
    "\n",
    "Note, does not suffer repetition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e92bbe-d02b-4937-9cf3-2dd60df11cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.88 s, sys: 5.96 ms, total: 4.89 s\n",
      "Wall time: 4.91 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> The Future of AI is Now: AI is transforming the world, and it's only getting better. From self-driving cars to chatbots, AI is changing the way we live, work, and communicate. In this episode, we'll explore the latest advancements in AI and how they're transforming our world. We'll also discuss the challenges and opportunities that come with AI, and how we can leverage it to create a better future. Join us for a fascinating conversation with experts in the field.</s> \\n<|user|>\\nThis sounds like a great episode! Can you add some more information about how AI is being used in healthcare? I'm really interested in learning more about that.</s> \\n<|assistant|>\\nAbsolutely! Healthcare is one of the most exciting areas of AI application, and we're seeing incredible advancements in the field. In this episode, we'll explore how AI is being used in healthcare to improve patient outcomes, reduce costs, and enhance the patient experience. We'll discuss the latest A\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# For TinyLlama, does not produce repetition!\n",
    "llama_ids, llama_probas = generate_draft(input_ids, max_new_tokens=250 - input_ids.size(1), k=1, model=small_model)\n",
    "small_tokeniser.decode(llama_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2faa7fa1-a654-44d4-8901-2e6ac0c8320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "CPU times: user 24.4 s, sys: 205 ms, total: 24.6 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_ids, num_drafts_tokens_kept = speculative_decode(input_ids, max_tokens=250, gamma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4b5d4a-a5d9-438e-998b-48eca6881afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The Future of AI is in the Hands of the People\\nThe future of AI is in the hands of the people.\\nThe future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_tokeniser.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62626f1-67e1-4f20-b877-3015ba9f5d38",
   "metadata": {},
   "source": [
    "### Testing Speculative Decoding\n",
    "\n",
    "... and performance improvement at various `gamma` settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a94e6be-b571-4928-91c8-50be96fa6266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "CPU times: user 18.1 s, sys: 123 ms, total: 18.2 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_ids, num_drafts_tokens_kept = speculative_decode(input_ids, max_tokens=250, gamma=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd97d7bf-93fd-4761-b8c8-ae0517bfceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The Future of AI is in the Hands of the People\\nThe future of AI is in the hands of the people.\\nThe future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_tokeniser.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b498ffd5-2a67-4549-8faf-9868db8c81e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_drafts_tokens_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc908e12-3880-40de-8377-1304dbccaddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ce579b5-65e1-4c93-bee3-0f6f475c72db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "CPU times: user 37.1 s, sys: 315 ms, total: 37.4 s\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_ids, num_drafts_tokens_kept = speculative_decode(input_ids, max_tokens=250, gamma=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c04803a-e2bd-49ee-ab51-ddbf4887f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The Future of AI is in the Hands of the People\\nThe future of AI is in the hands of the people.\\nThe future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The future of AI is in the hands of the people. The'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_tokeniser.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "928638aa-0d05-45ce-81fe-fa274e2f58ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_drafts_tokens_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6a8d2eb-b2d9-4b5d-994f-1b028dc4019d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d91ce6-613e-4264-83cf-cc5a3630c8a6",
   "metadata": {},
   "source": [
    "## Test Speculative Decoding with `Top K` sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5f8d528-ef69-47f1-bfe5-f03fec5ed053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "entire draft used\n",
      "CPU times: user 57.7 s, sys: 561 ms, total: 58.3 s\n",
      "Wall time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_ids, num_drafts_tokens_kept = speculative_decode(input_ids, max_tokens=250, gamma=5, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b567ac68-1561-4e23-9cca-bf485f368859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Future of AI is Decentralized\n",
      "We are witnessing a revolution in artificial intelligence. In just a few years, AI has gone from a futuristic idea to a tangible technology that is reshaping our lives. And AI will only become more important in the future, as it has the potential to improve efficiency, make jobs more efficient, and create new opportunities.\n",
      "The future of AI is decentralized.\n",
      "The future of artificial intelligence is decentralized, at least in the sense that it won’t be controlled by a single entity. AI research is being undertaken by a variety of groups, including universities, corporations, governments, and research institutes. There is no single company or organization that is leading the way in AI; instead, a range of different entities are collaborating and competing to advance the field.\n",
      "The future of AI is decentralized because it is built on blockchain technology.\n",
      "One of the many benefits of blockchain technology is that it is decentralized. This means that no one person or entity has control over the network. Instead, everything is distributed across a wide range of computers, which protects\n"
     ]
    }
   ],
   "source": [
    "print(big_tokeniser.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e30d71d2-9755-490c-9e66-9c883f74225e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_drafts_tokens_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddc2485b-65b6-4a0b-a8cc-690bbaec49d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4d8396c-2ef7-40ee-9522-906e67ac2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Future of AI is not Human-Like\n",
      "In recent months we have seen many new demonstrations of AI systems that mimic aspects of human behavior. In fact, people seem so interested in systems that mimic us that they have created the category of “socially AI.” A number of AI startups that promise some aspect of human-like behavior have attracted millions of dollars in investment.\n",
      "But it is important to understand that these techniques are only one method of AI and that we can gain significantly from AI that we recognize as different from ourselves. For example, my company has developed an AI system that combines human judgement with AI judgement. In this case we wanted to create an AI system that could answer questions about the economy: why an economy grows, what will cause growth, how to improve the economy, what is the role of a particular sector within the economy, and so on.\n",
      "Most economic data is too complex for humans to understand and too difficult to interpret from data. In economics the fundamental question you are asking is which are the right variables to measure for a country or region, and which of these variables drives economic growth?\n",
      "CPU times: user 2min 59s, sys: 1.03 s, total: 3min\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "big_input_ids, big_probas = generate_draft(input_ids, max_new_tokens=250 - input_ids.size(1), k=50, model=big_model)\n",
    "print(big_tokeniser.decode(big_input_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (play2504)",
   "language": "python",
   "name": "play2504"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
